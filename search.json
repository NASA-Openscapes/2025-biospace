[
  {
    "objectID": "tutorials/lvis/lvis.html",
    "href": "tutorials/lvis/lvis.html",
    "title": "BIOSPACE25 Workshop:",
    "section": "",
    "text": "Michele Thornton, Rupesh Shrestha, Erin Hestir, Adam Wilson, Jasper Slingsby, Anabelle Cardoso\nDate: February 12, 2025, Frascati (Rome), Italy\n\n\n\nBIOSPACE25",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#harnessing-analysis-tools-for-biodiversity-applications-using-field-airborne-and-orbital-remote-sensing-data-from-nasas-bioscape-campaign",
    "href": "tutorials/lvis/lvis.html#harnessing-analysis-tools-for-biodiversity-applications-using-field-airborne-and-orbital-remote-sensing-data-from-nasas-bioscape-campaign",
    "title": "BIOSPACE25 Workshop:",
    "section": "",
    "text": "Michele Thornton, Rupesh Shrestha, Erin Hestir, Adam Wilson, Jasper Slingsby, Anabelle Cardoso\nDate: February 12, 2025, Frascati (Rome), Italy\n\n\n\nBIOSPACE25",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#overview",
    "href": "tutorials/lvis/lvis.html#overview",
    "title": "BIOSPACE25 Workshop:",
    "section": "Overview",
    "text": "Overview\nBioSCape, the Biodiversity Survey of the Cape, is NASA’s first biodiversity-focused airborne and field campaign that was conducted in South Africa in 2023. BioSCape’s primary objective is to study the structure, function, and composition of the region’s ecosystems, and how and why they are changing.\nBioSCape’s airborne dataset is unprecedented, with AVIRIS-NG, PRISM, and HyTES imaging spectrometers capturing spectral data across the UV, visible and infrared at high resolution and LVIS acquiring coincident full-waveform lidar. BioSCape’s field dataset is equally impressive, with 18 PI-led projects collecting data ranging from the diversity and phylogeny of plants, kelp and phytoplankton, eDNA, landscape acoustics, plant traits, blue carbon accounting, and more\nThis tutorial will demonstrate accessing and visualizing Land, Vegetation, and Ice Sensor (LVIS) data available on the BioSCape SMCE. LVIS is an airborne, wide-swath imaging full-waveform laser altimeter. LVIS collects in a 1064 nm-wavelength (near infrared) range with 3 detectors mounted in an airborne platform, flown typically ~10 km above the ground producing a data swath of 2km wide with 7-10 m footprints. More information about the LVIS instrument is here.\nThe LVIS instrument has been flown above several regions of the world since 1998. The LVIS flights were collected for the BioSCAPE campaigns from 01 Oct 2023 through 30 Nov 2023.\nThere are three LVIS BioSCape data products currently available and archived at NSIDC DAAC\n\n\n\nDataset\nDataset DOI\n\n\n\n\nLVIS L1A Geotagged Images V001\n10.5067/NE5KKKBAQG44\n\n\nLVIS Facility L1B Geolocated Return Energy Waveforms V001\n10.5067/XQJ8PN8FTIDG\n\n\nLVIS Facility L2 Geolocated Surface Elevation and Canopy Height Product V001\n10.5067/VP7J20HJQISD\n\n\n\nFor this tutorial, we’ll examine the LVIS L2 Geolocated Surface Elevation and Canopy Height Product V001\n\nCitation: Blair, J. B. & Hofton, M. (2020). LVIS Facility L2 Geolocated Surface Elevation and Canopy Height Product, Version 1 [Data Set]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/VP7J20HJQISD.\nUser Guide LVIS Facility L2 Geolocated Surface Elevation and Canopy Height Product, Version1\n\n Figure: Sample LVIS product waveforms illustrating possible distributions of reflected light (Blair et al., 2020)\n\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport earthaccess\nimport geopandas as gpd\nimport requests as re\nimport s3fs\nimport h5py\nfrom os import path\nfrom datetime import datetime\nfrom shapely.ops import orient\nfrom shapely.geometry import Polygon, MultiPolygon\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import box\nimport dill\nfrom harmony import BBox,Client, Collection, Request, LinkType\n\n\n# !pip install crepes dill xgboost pynndescent\n\n\n# esri background basemap for maps\nxyz = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nattr = \"ESRI\"",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#authentication",
    "href": "tutorials/lvis/lvis.html#authentication",
    "title": "BIOSPACE25 Workshop:",
    "section": "Authentication",
    "text": "Authentication\nWe recommend authenticating your Earthdata Login (EDL) information using the earthaccess python library as follows:\n\n# works if the EDL login already been persisted to a netrc\ntry:\n    auth = earthaccess.login(strategy=\"netrc\")\nexcept FileNotFoundError:\n    # ask for EDL credentials and persist them in a .netrc file\n    auth = earthaccess.login(strategy=\"interactive\", persist=True)",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#structure-lvis-l1b-geolocated-waveform",
    "href": "tutorials/lvis/lvis.html#structure-lvis-l1b-geolocated-waveform",
    "title": "BIOSPACE25 Workshop:",
    "section": "Structure LVIS L1B Geolocated Waveform",
    "text": "Structure LVIS L1B Geolocated Waveform\nFirst, let’s find out how many L1B files from the BioSCape Campaign are published on NASA Earthdata.\n\ndoi=\"10.5067/XQJ8PN8FTIDG\" # LVIS L1B doi\nquery = earthaccess.DataGranules().doi(doi)\nquery.params['campaign'] = 'bioscape'\nl1b = query.get_all()\nprint(f'Granules found: {len(l1b)}')\n\nThere are 2328 granules LVIS L1B product found for the time period. Let’s see if they are hosted on Earthdata cloud or not, by printing a summary of the first granule\n\nl1b[0]\n\nAs we see above, the LVIS files are not cloud-hosted yet. They have to be downloaded locally and use them. We will go ahead and use earthaccess python module to get the file directly and save it to the downloads directory.\n\n# download files\ndownloaded_files = earthaccess.download(l1b[0], local_path=\"downloads\")\n\nNow, we go ahead and open a file to look at its structure.\n\nl1b_first = f'downloads/{path.basename(l1b[0].data_links()[0])}'\nwith h5py.File(l1b_first) as hf:\n    for v in list(hf.keys()):\n        if v != \"ancillary_data\":\n            print(f\"- {v} : {hf[v].attrs['description'][0].decode('utf-8')}\")\n\n\nwith h5py.File(l1b_first) as hf:\n    l_lat = hf['LAT1215'][:]\n    l_lon = hf['LON1215'][:]\n    l_range = hf['Z1215'][:] # ground elevation\n    shot = hf['SHOTNUMBER'][:] # ground elevation\ngeo_arr = list(zip(shot, l_range,l_lat,l_lon))\nl1b_df = pd.DataFrame(geo_arr, columns=[\"shot_number\", \"elevation\", \"lat\", \"lon\"])\nl1b_df\n\n\nl1b_gdf = gpd.GeoDataFrame(l1b_df, crs=\"EPSG:4326\",\n                           geometry=gpd.points_from_xy(l1b_df.lon, \n                                                       l1b_df.lat))\nl1b_gdf.sample(frac=0.01).explore(\"elevation\", cmap = \"plasma\", tiles=xyz, attr=attr)\n\n\ni = np.where(shot==1319816)[0][0]\nwith h5py.File(l1b_first) as hf:\n    rxwaveform = hf['RXWAVE'][i, :] # waveform\n    elev_1215 = hf['Z1215'][i] # elevation ground\n    elev_0 = hf['Z0'][i] # elevation top\nc = len(rxwaveform) #\nelev = np.linspace(elev_1215, elev_0, num=c)\n# plot\nplt.rcParams[\"figure.figsize\"] = (3,10)\nplt.xlabel(\"rxwaveform (counts)\")\nplt.ylabel(\"elevation (m)\")\nplt.plot(rxwaveform, elev, linestyle='--', marker='.',)\nplt.show()",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#structure-of-lvis-l2-canopy-height-product",
    "href": "tutorials/lvis/lvis.html#structure-of-lvis-l2-canopy-height-product",
    "title": "BIOSPACE25 Workshop:",
    "section": "Structure of LVIS L2 Canopy Height Product",
    "text": "Structure of LVIS L2 Canopy Height Product\nFirst, let’s search how many L2 files are there for the BioSCape campaign dates.\n\ndoi=\"10.5067/VP7J20HJQISD\" # LVIS L2 doi\nquery = earthaccess.DataGranules().doi(doi)\nquery.params['campaign'] = 'bioscape'\nl2 = query.get_all()\nprint(f'Granules found: {len(l2)}')\n\nThere are 2328 granules LVIS L2 product found for the time period. Let’s see if they are hosted on Earthdata cloud or not, by printing a summary of the first granule\n\nl2[0]\n\nAs we see above, the LVIS files are not cloud-hosted yet. They have to be downloaded locally and use them. We will go ahead and use earthaccess python module to get the file directly and save it to the downloads directory.\n\n# download files\ndownloaded_files = earthaccess.download(l2[0], local_path=\"downloads\")\n\nNow, we’ll go ahead and open a file to look at it’s structure.\n\nl2_first = f'downloads/{path.basename(l2[0].data_links()[0])}'\nwith open(l2_first, 'rb') as f:\n    head = [next(f) for _ in range(15)]\n    print(head)\n\nLVIS L2A files have a number of lines as header as shown above. Let’s define a python function to read the number of header rows.\n\ndef get_line_number(filename):\n    \"\"\"find number of header rows in LVIS L2A\"\"\"\n    count = 0\n    with open(filename, 'rb') as f:\n        for line in f:\n            if line.startswith(b'#'):\n                count = count + 1\n                columns = line[1:].split()\n            else:\n                return count, columns\n\n\nh_no, col_names = get_line_number(l2_first)\nwith open(l2_first, 'rb') as f:\n    l2a_df = pd.read_csv(f, skiprows=h_no, header=None, engine='python', sep=r'\\s+')\n    l2a_df.columns =  [x.decode() for x in col_names]\nl2a_df\n\nLet’s print LVIS L2 variable names.\n\nl2a_df.columns.values\n\nKey variables: - The GLAT and GLON variables provide coordinates of the lowest detected mode within the LVIS waveform. - The RHXX variables provide height above the lowest detected mode at which XX percentile of the waveform energy. - RANGE provides the distance from the instrument to the ground. - SENSITIVITY provides sensitivity metric for the return waveform. - ZG,ZG_ALT1, ZG_ALT2: Mean elevation of the lowest detected mode using alternate 1/2 mode detection settings. The alternative ZG to use to adjust the RH parameters for local site conditions.\nMore information about the variables are provided in the user guide.\nLet’s select the particular shot we are interested in.\n\nl2a_shot_df = l2a_df[l2a_df['SHOTNUMBER'] == 1319816]\nl2a_shot_df\n\nNow, we can plot the RH metrics with elevation.\n\nelev_zg = l2a_shot_df.ZG.values[0]\nelev_zh = l2a_shot_df.ZH.values[0]\nrh40 = l2a_shot_df.RH40.values[0]\nrh80 = l2a_shot_df.RH80.values[0]\nrh100 = l2a_shot_df.RH100.values[0]\nrh = l2a_shot_df.filter(like ='RH').drop('CHANNEL_RH', axis=1).values.tolist()[0]\n\n# plotting\nplt.rcParams[\"figure.figsize\"] = (5,7)\nrh_vals = l2a_shot_df.filter(like = 'RH').columns[:-1].str.strip('RH').astype(int).tolist()\nfig, ax1 = plt.subplots()\nax1.plot(rh_vals, elev_zg+rh, alpha=0.3, marker='o', color='black', label='RH metrics' )\nax1.axhline(y=elev_zg+rh40,  color='g', linestyle='dotted', linewidth=2, label='RH40')\nax1.axhline(y=elev_zg+rh80,  color='g', linestyle='-.', linewidth=2, label='RH80')\nax1.axhline(y=elev_zg+rh100,  color='g', linestyle='--', linewidth=3, label='RH100')\nax1.set_xlabel(\"Percentile of waveform energy (%)\")\nax1.set_ylabel(\"elevation (meters)\")\nax1.legend(loc=\"lower right\")\nplt.show()",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#plot-lvis-bioscape-campaigns",
    "href": "tutorials/lvis/lvis.html#plot-lvis-bioscape-campaigns",
    "title": "BIOSPACE25 Workshop:",
    "section": "Plot LVIS Bioscape Campaigns",
    "text": "Plot LVIS Bioscape Campaigns\nLets first define two functions that we can use to plot the search results above over a basemap.\n\ndef convert_umm_geometry(gpoly):\n    \"\"\"converts UMM geometry to multipolygons\"\"\"\n    multipolygons = []\n    for gl in gpoly:\n        ltln = gl[\"Boundary\"][\"Points\"]\n        points = [(p[\"Longitude\"], p[\"Latitude\"]) for p in ltln]\n        multipolygons.append(Polygon(points))\n    return MultiPolygon(multipolygons)\n\ndef convert_list_gdf(datag):\n    \"\"\"converts List[] to geopandas dataframe\"\"\"\n    # create pandas dataframe from json\n    df = pd.json_normalize([vars(granule)['render_dict'] for granule in datag])\n    # keep only last string of the column names\n    df.columns=df.columns.str.split('.').str[-1]\n    # convert polygons to multipolygonal geometry\n    df[\"geometry\"] = df[\"GPolygons\"].apply(convert_umm_geometry)\n    # return geopandas dataframe\n    return gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n\nNow, we will convert the JSON return from the earthaccess granule search into a geopandas dataframe so we could plot over an ESRI basemap.\n\ngdf = convert_list_gdf(l2)\n#plot\ngdf[['BeginningDateTime','geometry']].explore(tiles=xyz, attr=attr)\n\nThe above shows the flight lines of all LVIS collection during the BioSCape campaign in 2023.\n\nSearch LVIS L2A granules over a study area\nThe study area is Brackenburn Private Nature Reserve.\n\npoly_f = \"assets/brackenburn.json\"\npoly = gpd.read_file(poly_f)\npoly.explore(tiles=xyz, attr=attr, style_kwds={'color':'red', 'fill':False})\n\nLet’s search for LVIS L2A granules that are within the bounds of the study area.\n\npoly.geometry = poly.geometry.apply(orient, args=(1,))\n# simplifying the polygon to bypass the coordinates \n# limit of the CMR with a tolerance of .005 degrees\nxy = poly.geometry.simplify(0.005).get_coordinates()\n\ngranules = earthaccess.search_data(\n    count=-1, # needed to retrieve all granules\n    doi=\"10.5067/VP7J20HJQISD\", # LVIS L2A doi\n    temporal=(\"2023-10-01\", \"2023-11-30\"), # Bioscape campaign dates\n    polygon=list(zip(xy.x, xy.y))\n)\nprint(f'Granules found: {len(granules)}')\n\nLet’s check when these LVIS flights were flown.\n\ngdf = convert_list_gdf(granules)\ngdf['BeginningDateTime'] = pd.to_datetime(gdf['BeginningDateTime'])\ngdf_daily = gdf.resample(rule='D', on='BeginningDateTime')['concept-id'].nunique()\ngdf_daily.index = pd.to_datetime(gdf_daily.index).date\ngdf_daily.plot(kind=\"bar\", xlabel=\"Day of Flight\", ylabel=\"No. of flights\")\nplt.show()\n\nLet’s plot these flight lines over a basemap.\n\nm = gdf[['BeginningDateTime','geometry']].explore(tiles=xyz, attr=attr, \n                                              style_kwds={'fillOpacity':0.1})\nm\n\n\n\nDownload L2 granules\nIn the above section, we retrieved the overlapping granules using the earthaccess module, which we will also use to download the files.\n\ndownloaded_files = earthaccess.download(granules, local_path=\"downloads\")\n\nEarthaccess download uses a parallel downloading feature so all the files are downloaded efficiently. It also avoids duplicated downloads if a file has already been downloaded previously to the local path.\n\n\nRead LVIS L2A Files\nNow, we can read the LVIS L2A files into a pandas dataframe.\n\n# read the LVIS L2A files\nlvis_l2a = []\nfor s in gdf.index:\n    f = path.join('downloads', path.basename(granules[s].data_links()[0]))\n    h_no, col_names = get_line_number(f)\n    temp_df = pd.read_csv(f, skiprows=h_no, header=None, \n                          engine='python', sep=r'\\s+')\n    temp_df.columns =  [x.decode() for x in col_names]\n    temp_gdf = gpd.GeoDataFrame(temp_df, \n                                geometry=gpd.points_from_xy(temp_df.GLON, \n                                                            temp_df.GLAT),\n                                crs=\"EPSG:4326\")\n    temp_sub = gpd.sjoin(temp_gdf, poly, predicate='within')\n    if not temp_sub.empty:\n        print(f'Subsetting {path.basename(f)}')\n        lvis_l2a.append(temp_sub)\nlvis_l2a_gdf = pd.concat(lvis_l2a)\n\n\n\nHeight of Top Canopy\nLet’s plot RH100 (height of top canopy) values in a map.\n\nlvis_l2a_gdf[['RH100', 'geometry']].sample(frac=0.1, random_state=1).explore(\n    \"RH100\", cmap = \"YlGn\", tiles=xyz, attr=attr, alpha=0.5, radius=10, legend=True)\n\n\n\nRelative Height Distribution\nWe can generate a plot of RH metrics to check if the vegetation height across the percentile of waveform energy indicates the same.\n\nfig, ax = plt.subplots(figsize=(7, 8))\nplot_df = lvis_l2a_gdf.sample(frac=0.1, random_state=1).filter(like='RH').drop('CHANNEL_RH', axis=1).T\nplot_df.index = plot_df.index.str.strip('RH').astype(int)\nstd_df = plot_df.std(axis=1)\nmedian_df = plot_df.median(axis=1)\nmedian_df.plot(ax=ax, alpha=0.5, style='o-')\nax.fill_between(plot_df.index, median_df - std_df, median_df + std_df, alpha=0.1)\nax.set_xlabel(\"Percentile of waveform energy (%)\")\nax.set_ylabel(\"Height (m)\")\nplt.show()\n\n\n# exporting \n# lvis_l2a_gdf.geometry.to_file('lvis2.geojson', driver=\"GeoJSON\")\n# lvis_l2a_gdf.to_csv('lvis.csv')",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#gedi-l2a-canopy-height-metrics",
    "href": "tutorials/lvis/lvis.html#gedi-l2a-canopy-height-metrics",
    "title": "BIOSPACE25 Workshop:",
    "section": "GEDI L2A Canopy Height Metrics",
    "text": "GEDI L2A Canopy Height Metrics\nThis section of the tutorial will demonstrate how to directly access and subset the GEDI L2A canopy height metrics using NASA’s Harmony Services and compute a summary of aboveground biomass density for a forest reserve. The Harmony API allows seamless access and production of analysis-ready Earth observation data across different DAACs by enabling cloud-based spatial, temporal, and variable subsetting and data conversions. The GEDI datasets are available from the Harmony API.\nWe will use NASA’s Harmony Services to retrieve the GEDI L2A dataset and canopy heights (RH100) for the burned and unburned plots . The Harmony API allows access to selected variables for the dataset within the spatial-temporal bounds without having to download the whole data file.\n\nDataset\nThe GEDI Level 2A Geolocated Elevation and Height Metrics product GEDI02_A provides waveform interpretation and extracted products from eachreceived waveform, including ground elevation, canopy top height, and relative height (RH) metrics. GEDI datasets are available for the period starting 2019-04-17 and covers 52 N to 52 S latitudes. GEDI L2A data files are natively in HDF5 format.\n\n\nAuthentication\nNASA Harmony API requires NASA Earthdata Login (EDL). You can use the earthaccess Python library to set up authentication. Alternatively, you can also login to harmony_client directly by passing EDL authentication as the following in the Jupyter Notebook itself:\nharmony_client = Client(auth=(\"your EDL username\", \"your EDL password\"))\n\n\nCreate Harmony Client Object\nFirst, we create a Harmony Client object. If you are passing the EDL authentication, please do as shown above with the auth parameter.\n\nharmony_client = Client()\n\n\n\nRetrieve Concept ID\nNow, let’s retrieve the Concept ID of the GEDI L2A dataset. The Concept ID is NASA Earthdata’s unique ID for its dataset.\n\ndef get_concept_id(doi):\n    \"\"\"get concept id from DOI using CMR API\"\"\"\n    doisearch = f'https://cmr.earthdata.nasa.gov/search/collections.json?doi={doi}' \n    return re.get(doisearch).json()['feed']['entry'][0]['id']\n\nconcept_l2a = get_concept_id('10.5067/GEDI/GEDI02_A.002') # GEDI L2A DOI\nprint(f\"{concept_l2a}\")\n\n\n\nDefine Request Parameters\nLet’s create a Harmony Collection object with the concept_id retrieved above. We will also define the GEDI L2A RH variables and temporal range.\n\n# harmony collection\ncollection_l2a = Collection(id=concept_l2a)\n\ndef create_var_names(variables):\n    # gedi beams\n    beams = ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']\n    # combine variables and beam names\n    return [f'/{b}/{v}' for b in beams for v in variables]\n\n# gedi variables\nvariables_l2a = create_var_names(['rh', 'quality_flag', 'land_cover_data/pft_class'])\n\n# time range\ntemporal_range = {'start': datetime(2019, 4, 17), \n                  'stop': datetime(2023, 3, 31)}\n\n\n\nCreate and Submit Harmony Request\nNow, we can create a Harmony request with variables, temporal range, and bounding box and submit the request using the Harmony client object. We will use the download_all method, which uses a multithreaded downloader and returns a concurrent future. Futures are asynchronous and let us use the downloaded file as soon as the download is complete while other files are still being downloaded.\n\ndef submit_harmony(collection, variables):\n    \"\"\"submit harmony request\"\"\"\n    request = Request(collection=collection, \n                  variables=variables, \n                  temporal=temporal_range,\n                  shape=poly_f,\n                  ignore_errors=True)\n    # submit harmony request, will return job id\n    subset_job_id = harmony_client.submit(request)\n    return harmony_client.result_urls(subset_job_id, show_progress=True, \n                                      link_type=LinkType.s3)\n\nresults = submit_harmony(collection_l2a, variables_l2a)\n\nA temporary S3 Credentials is needed for read-only, same-region (us-west-2), direct access to S3 objects on the Earthdata cloud. We will use the credentials from the harmony_client.\n\ns3credentials = harmony_client.aws_credentials()\n\nWe will pass S3 credentials to S3Fs class S3FileSystem.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=s3credentials['aws_access_key_id'], \n                          secret=s3credentials['aws_secret_access_key'], \n                          token=s3credentials['aws_session_token'])\n\n\n\nRead Subset files\nFirst, we will define a python function to read the GEDI variables, which are organized in a hierarchical way.\n\ndef read_gedi_vars(beam):\n    \"\"\"reads through gedi variable hierarchy\"\"\"\n    col_names = []\n    col_val = []\n    # read all variables\n    for key, value in beam.items():\n        # check if the item is a group\n        if isinstance(value, h5py.Group):\n            # looping through subgroups\n            for key2, value2 in value.items():\n                col_names.append(key2)\n                col_val.append(value2[:].tolist())\n        else:\n            col_names.append(key)\n            col_val.append(value[:].tolist())\n    return col_names, col_val\n\nLet’s direct access the subsetted h5 files and retrieve its values into the pandas dataframe.\n\n# define an empty pandas dataframe\nsubset_df = pd.DataFrame()\n# loop through the Harmony results\nfor s3_url in results:\n    print(s3_url)\n    with fs_s3.open(s3_url, mode='rb') as fh:\n        with h5py.File(fh) as l2a_in:\n            for v in list(l2a_in.keys()):\n                if v.startswith('BEAM'):\n                    c_n, c_v = read_gedi_vars(l2a_in[v])\n                    # Appending to the subset_df dataframe\n                    subset_df = pd.concat([subset_df, \n                                           pd.DataFrame(map(list, zip(*c_v)), \n                                                        columns=c_n)])\n\nLet’s remove the duplicate columns, if any, and print the first two rows of the pandas dataframe.\n\n# remove duplicate columns\nsubset_df = subset_df.loc[:,~subset_df.columns.duplicated()].copy()\nsubset_df.head(2)\n\n\n\nRH metrics\nLet’s first split the rh variables into different columns. There are a total of 101 steps of percentiles defined starting from 0 to 100%.\n\n# create rh metrics column\nrh = []\n# loop through each percentile and create a RH column\nfor i in range(101):\n    y = pd.DataFrame({f'RH{i}':subset_df['rh'].apply(lambda x: x[i])})\n    rh.append(y)\nrh = pd.concat(rh, axis=1)\n\n# concatenate RH columns to the original dataframe\nsubset_df = pd.concat([subset_df, rh], axis=1)\n# print the first row of dataframe\nsubset_df.head(2)\n\n\ngedi_gdf = gpd.GeoDataFrame(subset_df, \n                                geometry=gpd.points_from_xy(subset_df.lon_lowestmode, \n                                                            subset_df.lat_lowestmode),\n                                crs=\"EPSG:4326\")\ngedi_sub = gpd.sjoin(gedi_gdf, poly, predicate='within')\ngedi_sub[gedi_sub.quality_flag==1].explore(\"RH100\", cmap = \"YlGn\", tiles=xyz, attr=attr, alpha=0.5, radius=10, legend=True)\n\n\nfig, ax = plt.subplots(figsize=(7, 10))\nplot_df = gedi_sub[gedi_sub.quality_flag==1].filter(like='RH').T\nplot_df.index = plot_df.index.str.strip('RH').astype(int)\nplot_df.median(axis=1).plot(ax=ax, alpha=0.5, style='s:')\nax.set_xlabel(\"Percentile of waveform energy (%)\")\nax.set_ylabel(\"Height (m)\")\nplt.show()",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/lvis/lvis.html#gedi-waveform-structural-complexity-index-wsci",
    "href": "tutorials/lvis/lvis.html#gedi-waveform-structural-complexity-index-wsci",
    "title": "BIOSPACE25 Workshop:",
    "section": "GEDI Waveform Structural Complexity Index (WSCI)",
    "text": "GEDI Waveform Structural Complexity Index (WSCI)\nGEDI WSCI product (GEDI L4C) provides inference on forest structural complexity, which affects ecosystem functions, nutrient cycling, biodiversity, and habitat quality (Tiago et al. 2024)\n\nconcept_l4c = get_concept_id('10.3334/ORNLDAAC/2338') # GEDI L4C DOI\n# harmony collection\ncollection_l4c = Collection(id=concept_l4c)\n# gedi variables\nvariables_l4c = create_var_names(['wsci'])\n# define an empty pandas dataframe\nsubset_df2 = pd.DataFrame()\n# submit harmony\nresults2 = submit_harmony(collection_l4c, variables_l4c)\n# loop through the Harmony results\nfor s3_url in results2:\n    print(s3_url)\n    with fs_s3.open(s3_url, mode='rb') as fh:\n        with h5py.File(fh) as l4c_in:\n            for v in list(l4c_in.keys()):\n                if v.startswith('BEAM'):\n                    c_n, c_v = read_gedi_vars(l4c_in[v])\n                    # Appending to the subset_df dataframe\n                    subset_df2 = pd.concat([subset_df2, \n                                           pd.DataFrame(map(list, zip(*c_v)), \n                                                        columns=c_n)])\n\n\n# remove duplicate columns\nsubset_df2 = subset_df2.loc[:,~subset_df2.columns.duplicated()].copy()\ngedi_gdf2 = gpd.GeoDataFrame(subset_df2, \n                                geometry=gpd.points_from_xy(subset_df2.lon_lowestmode, \n                                                            subset_df2.lat_lowestmode),\n                                crs=\"EPSG:4326\")\ngedi_sub2 = gpd.sjoin(gedi_gdf2, poly, predicate='within')\ngedi_sub2[gedi_sub2.quality_flag==1].explore(\"WSCI\", cmap = \"YlGn\", tiles=xyz, attr=attr, alpha=0.5, radius=10, legend=True)",
    "crumbs": [
      "LVIS/GEDI Data for Ecosystem Structure",
      "LVIS/GEDI Data"
    ]
  },
  {
    "objectID": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html",
    "href": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html",
    "title": "BIOSPACE25 Workshop:",
    "section": "",
    "text": "Michele Thornton, Rupesh Shrestha, Erin Hestir, Adam Wilson, Jasper Slingsby, Anabelle Cardoso\nDate: February 12, 2025, Frascati (Rome), Italy\n\n\n\nBIOSPACE25",
    "crumbs": [
      "Mapping invasive species using AVIRIS-NG",
      "Machine learning with AVIRIS-NG"
    ]
  },
  {
    "objectID": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html#harnessing-analysis-tools-for-biodiversity-applications-using-field-airborne-and-orbital-remote-sensing-data-from-nasas-bioscape-campaign",
    "href": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html#harnessing-analysis-tools-for-biodiversity-applications-using-field-airborne-and-orbital-remote-sensing-data-from-nasas-bioscape-campaign",
    "title": "BIOSPACE25 Workshop:",
    "section": "",
    "text": "Michele Thornton, Rupesh Shrestha, Erin Hestir, Adam Wilson, Jasper Slingsby, Anabelle Cardoso\nDate: February 12, 2025, Frascati (Rome), Italy\n\n\n\nBIOSPACE25",
    "crumbs": [
      "Mapping invasive species using AVIRIS-NG",
      "Machine learning with AVIRIS-NG"
    ]
  },
  {
    "objectID": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html#overview",
    "href": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html#overview",
    "title": "BIOSPACE25 Workshop:",
    "section": "Overview",
    "text": "Overview\nIn this notebook, we will use existing data of verified land cover and alien species locations to extract spectra from AVIRIS NG surface reflectance data.",
    "crumbs": [
      "Mapping invasive species using AVIRIS-NG",
      "Machine learning with AVIRIS-NG"
    ]
  },
  {
    "objectID": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html#learning-objectives",
    "href": "tutorials/avirisng/avng_invasive_esaworkshop_noS3.html#learning-objectives",
    "title": "BIOSPACE25 Workshop:",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand how to inspect and prepare data for machine learning models\nTrain and interpret a machine learning model\nApply a trained model to AVIRIS imagery to create alien species maps\n\n\nLoad Python Modules\n\n#!pip install --user xvec\n#!pip install --user shap\n#!pip install --user xgboost\n\n\nfrom os import path\nimport geopandas as gpd\nimport s3fs\nimport pandas as pd\nimport xarray as xr\nfrom shapely.geometry import box, mapping\nimport rioxarray as riox\nimport numpy as np\nimport netCDF4 as nc\nimport hvplot.xarray\nimport holoviews as hv\nimport xvec\nimport matplotlib.pyplot as plt\nfrom dask.diagnostics import ProgressBar\nimport warnings\n#our functions\nfrom utils import get_first_xr\n\nwarnings.filterwarnings('ignore')\nhvplot.extension('bokeh')\n\n\n\nExplore Sample Land Type Plot-Level Data\nA small dataset over the Cape Town Peninsula of South Africa of manually collected invasive plant and land cover label - ct_invasive.gpkg\n\n# let's create a DataFrame and assign labels to each class\n\nlabel_df = pd.DataFrame({'LandType': ['Bare ground/Rock','Mature Fynbos', \n              'Recently Burnt Fynbos', 'Wetland', \n              'Forest', 'Pine', 'Eucalyptus' , 'Wattle', 'Water'],\n               'class': ['0','1','2','3','4','5','6','7','8']})\n\nlabel_df\n\n\n# open the dataset and project to the South African UTM projection also used by the AVIRIS-NG airborne data \nclass_data = gpd.read_file('data/ct_invasive.gpkg')\n# class_data.crs\nclass_data_utm = (class_data\n                 .to_crs(\"EPSG:32734\")\n                 .merge(label_df, on='class', how='left')\n                 )\nclass_data_utm\n\n\n\nSummarize and Visualize the Land Types\n\nclass_data_utm.groupby(['LandType']).size()\n\n\nclass_data_utm.groupby(['group']).size()\n\n\n# Let's visualize the plot data in an interactive map, with color by class, using a Google satellite basemap\nmap = class_data_utm[['LandType', 'geometry']].explore('LandType', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\nmap\n\n\n\nAVIRIS-NG Data over Cape Town Peninsula\nThere is a coverage file that has the bounding box of each AVIRIS-NG flight scene made available by the BioSCape Science Team. - ANG_Coverage.geojson\nFlight lines are provided as smaller sections within each flight line. We’ll refer to these smaller sections as scences. The data for each scene within a flight line is seamless to the adjacent scenes.\n\n# read and plot the AVNG coverage file\nAVNG_Coverage = gpd.read_file('data/ANGv2_Coverage.geojson', driver='GeoJSON')\nAVNG_Coverage.keys()\n\n\nnote that the ‘RFL s3’ key was pre-populated in the geojson file!!\nwe’ll see this s3 file list in an upcoming list\n\n\nAVNG_Coverage.crs\n\n\nAVNG_Coverage\n\n\n# Let's visualize the plot data in an interactive map, with color by class, using a Google satellite basemap\nmap = AVNG_Coverage[['fid', 'geometry']].explore(tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n#map = AVNG_Coverage[['fid', 'geometry']].explore('fid')\nmap\n\n\nAVIRIS-NG Principle Investigator Researchers are finalizing formats and standards of AVIRIS-NG airborne radiance and reflectance files. When finalized, the data will be published to into NASA Earthdata.\nFor now, JPL provides preliminatry AVIRIS-NG data here. Once finalized, AVIRIS-NG data from the BioSCape Campaign will be available from NASA Earthdata Cloud Storage.\n\n\n# Workshop participants will download this file from JPL\n# If you need to download this file, uncomment the wget line and run this code block.\n# !wget https://popo.jpl.nasa.gov/pub/bioscape_netCDF/rfl/ang20231109t133124_005_L2A_OE_0b4f48b4_RFL_ORT.nc -P /home/jovyan/2025-biospace/tutorials/avirisng/data/ang\n\n\n\nSelect the AVIRIS-NG Flight Line data to selected parameters and create lists to use later\nFor our analysis demonstration in this Notebook, we’ll narrow the flight lines to the area of the Cape Penisula and for flights that took place on 2023-11-09. - the Python GeoDataFrame.to_crs method Transform geometries to a new coordinate reference system.\n\n# temporal filter:  filter dates to between midnight on 2023-11-09 and 23:59:59 on 2023-11-09\nAVNG_CP = AVNG_Coverage[(AVNG_Coverage['end_time'] &gt;= '2023-11-09 00:00:00') & (AVNG_Coverage['end_time'] &lt;= '2023-11-09 23:59:59')]\nAVNG_CP = AVNG_CP.to_crs(\"EPSG:32734\")\n\n#keep only AVNG_CP that intersects with class_data\nAVNG_CP = AVNG_CP[AVNG_CP.intersects(class_data_utm.unary_union)]\n#AVNG_CP\n\nfiles_s3 = AVNG_CP['RFL s3'].tolist()\nfiles_AVNG_geo = AVNG_CP['geometry'].tolist()\nfiles_AVNG_geo\n\n#Visualize the selected flight lines\n#m = AVNG_CP[['fid','geometry']].explore('fid')\nm = AVNG_CP[['fid', 'geometry']].explore('fid', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n#explore('LandType', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\nm\n\n\nAVNG_CP.to_file('AVNG_CP.geojson', driver='GeoJSON')\n\n\nAVNG_CP.crs\n\n\nprint(AVNG_CP['fid'])\n\n\nfiles_s3[26]\n\n\nThe AVIRIS-NG files are also in S3 buckets in a BioSCape Science Managed Cloud Environment (SMCE).\n\nSMCE’s support NASA Funded researchers by providing a secure hub to store and analyze data. These SMCE’s are in AWS US-West. Workshop instructors are able to access these files.\n\n\n\nS3 access is commented out for workshop participants\n\n# Using BioSCape AWS Credentials to acces BioSCape SMCE\n# import s3fs\n# secret_key=\n# access_key=\n# token =\n# fs = s3fs.S3FileSystem(anon=False, \n#     secret=secret_key,\n#     key=access_key,\n#     token=token)\n\n\n\n\nExplore the BioSCape S3 Data Holdings\n\nS3 = Amazon Simple Storage Service (S3) is a cloud storage service that allows users to store and retrieve data\nS3 Bucket = Buckets are the basic containers that hold data. Buckets can be likened to file folders and object storage\nS3Fs is a Pythonic open source tool that mounts S3 object storage locally. S3Fs provides a filesystem-like interface for accessing objects on S3. &gt;import s3fs &gt; &gt;fs = s3fs.S3FileSystem(anon=False)\nThe top-level class S3FileSystem holds connection information and allows typical file-system style operations like ls, cp, mv\n\nls is a UNIX command to list computer files and directories\n\n\n\n#fs.ls('bioscape-data/')\n\n\n#fs.ls('bioscape-data/AVNG_V2/')\n\n\n#fs.ls('bioscape-data/AVNG_V2/ang20231109t133124/ang20231109t133124_005')\n\n\nSingle AVIRIS-NG flight scene Reflectance file ang20231109t133124_005_L2A_OE_0b4f48b4_RFL_ORT.nc\n\n\n\nOpen a single AVIRIS-NG Reflectance file to inspect the data\n\nS3Fs can be used to mount S3 object storage locally\nxarray is an open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays\n\n\n## Sample code to open a file from an S3 bucket using S3Fs\n\n#rfl_netcdf = xr.open_datatree(fs.open(files_s3[26], 'rb'),\n#                              engine='h5netcdf', chunks={})\n\n\n\n# For this workshop, we're using a local AVIRIS-NG scence \nrfl_netcdf_2i2c = 'data/ang/ang20231109t134249_006_L2A_OE_0b4f48b4_RFL_ORT.nc'\nrfl_netcdf_2i2c\n\n\n#rfl_netcdf = xr.open_datatree(fs.open(files_s3[26], 'rb'),\n#                              engine='h5netcdf', chunks={})\n\nrfl_netcdf = xr.open_datatree(rfl_netcdf_2i2c, engine='h5netcdf', chunks={})\nrfl_netcdf = rfl_netcdf.reflectance.to_dataset()\nrfl_netcdf = rfl_netcdf.reflectance.where(rfl_netcdf.reflectance&gt;0)\nrfl_netcdf\n\n\n\nPlot a true color image\n\nh = rfl_netcdf.sel(wavelength=[660, 570, 480], method=\"nearest\").hvplot.rgb('easting', 'northing',\n                                                                            rasterize=True, data_aspect=1,\n                                                                            bands='wavelength', frame_width=400)\nh\n\n\n\nPlot just a red reflectance\n\nh = rfl_netcdf.sel({'wavelength': 660},method='nearest').hvplot('easting', 'northing',\n                                                      rasterize=True, data_aspect=1,\n                                                      cmap='magma',frame_width=400,clim=(0,0.3))\nh\n\n\n\nExtract Spectra for each Land Plot\n\nNow that we are familiar with the data, we want to get the AVIRIS spectra at each label location. Below is a function that does this and returns the result as a xarray\nRecall some files we created earlier: - files_s3 = list; S3 netCDF files directories from the Cape Penisula subset area - files_AVNG_geo = list; coordinates of bounding boxes of the flight line scenes from the Cape Penisula area - class_data_utm = gpd; Cape Penisula Land Types with UTM geography\n\n#the function takes a filepath to a file on s3, and the point locations for extraction\n#this function requires hitting files on the BioSCape SMCE\n\n# def extract_points(s3uri, geof, points):\n#     ds = xr.open_datatree(fs.open(s3uri, 'rb'), decode_coords='all',\n#                           engine='h5netcdf', chunks='auto')\n        \n#     # Clip the raw data to the bounding box  \n#     points = points.clip(geof)\n#     print(f'got {points.shape[0]} point from {s3uri}')\n#     points = points.to_crs(ds.transverse_mercator.crs_wkt)\n    \n        \n#     # Extract points\n#     #extracted = ds.to_dataset().xvec.extract_points(points['geometry'], x_coords=\"easting\", y_coords=\"northing\",index=True)\n#     extracted = ds.reflectance.to_dataset().xvec.extract_points(points['geometry'], \n#                                                                 x_coords=\"easting\", \n#                                                                 y_coords=\"northing\",\n#                                                                 index=True)\n#     return extracted\n\nWhen we call the function, we’ll iterate through the list of files (files_s3). Each file will overlap with several land class points.\n\n# ds_all = [extract_points(file, geo, class_data_utm) for file, geo in zip(files_s3, files_AVNG_geo)]\n# ds_all = xr.concat(ds_all, dim='file')\n\n\n#ds_all\n\nBecause some points are covered by multiple AVIRIS scenes, some points have multiple spectra for each location, and thus we have an extra dim in this. We will simply extract the first valid reflectance measurement for each geometry. We have a custom function to do this get_first_xr()\n\n# ds = get_first_xr(ds_all)\n# ds\n\nThis data set just has the spectra. We need to merge with point data to add labels\n\n# class_xr =class_data_utm[['class','group']].to_xarray()\n# ds = ds.merge(class_xr.astype(int),join='left')\n# ds\n\nWe have defined all the operations we want, but becasue of xarrays lazy compution, the calculations have not yet been done. We will now force it to perform this calculations. We want to keep the result in chunks, so we use .persist() and not .compute(). This should take approx 2 - 3 mins\n\n##  DUE TO RUN TIME LENGTH, WE WILL NOT RUN THIS IN THE WORKSHOP - HAVE SAVED THIS OUTPUT FOR NEXT STEP\n# with ProgressBar():\n# dsp = ds.persist()\n\n\ndsp = xr.open_dataset('dsp.nc')\ndsp\n\n\n\n\nInspect AVIRIS spectra\n\n# recall the class types\nlabel_df\n\n\ndsp_plot = dsp.where(dsp['class']==5, drop=True)\nh = dsp_plot['reflectance'].hvplot.line(x='wavelength',by='index',\n                                    color='green', alpha=0.5,legend=False)\nh\n\n\nAt this point in a real machine learning workflow, you should closely inspect the spectra you have for each class. Do they make sense? Are there some spectra that look weird? You should re-evaluate your data to make sure that the assigned labels are true. This is a very important step\n\n\nPrep data for ML model\nAs you will know, not all of the wavelengths in the data are of equal quality, some will be degraded by atmospheric water absorption features or other factors. We should remove the bands from the analysis that we are not confident of. Probably the best way to do this is to use the uncertainties provided along with the reflectance files. We will simply use some prior knowledge to screen out the worst bands.\n\nwavelengths_to_drop = dsp.wavelength.where(\n    (dsp.wavelength &lt; 450) |\n    (dsp.wavelength &gt;= 1340) & (dsp.wavelength &lt;= 1480) |\n    (dsp.wavelength &gt;= 1800) & (dsp.wavelength &lt;= 1980) |\n    (dsp.wavelength &gt; 2400), drop=True\n)\n\n# Use drop_sel() to remove those specific wavelength ranges\ndsp = dsp.drop_sel(wavelength=wavelengths_to_drop)\n\nmask = (dsp['reflectance'] &gt; -1).all(dim='wavelength')  # Create a mask where all values along 'z' are non-negative\ndsp = dsp.sel(index=mask)\ndsp\n\nNext we will normalize the data, there are a number of difference normalizations to try. In a ML workflow you should try a few and see which work best. We will only use a Brightness Normalization. In essence, we scale the reflectance of each wavelength by the total brightness of the spectra. This retains info on important shape features and relative reflectance, and removes info on absolute reflectance.\n\n# Calculate the L2 norm along the 'wavelength' dimension\nl2_norm = np.sqrt((dsp['reflectance'] ** 2).sum(dim='wavelength'))\n\n# Normalize the reflectance by dividing by the L2 norm\ndsp['reflectance'] = dsp['reflectance'] / l2_norm\n\nPlot the new, clean spectra\n\ndsp_norm_plot = dsp.where(dsp['class']==5, drop=True)\nh = dsp_norm_plot['reflectance'].hvplot.line(x='wavelength',by='index',\n                                         color='green',ylim=(-0.01,0.2),alpha=0.5,legend=False)\nh\n\n\n\n\nTrain and evaluate the ML model\nWe will be using a model called xgboost. There are many, many different kinds of ML models. xgboost is a class of models called gradient boosted trees, related to random forests. When used for classification, random forests work by creating multiple decision trees, each trained on a random subset of the data and features, and then averaging their predictions to improve accuracy and reduce overfitting. Gradient boosted trees differ in that they build trees sequentially, with each new tree focusing on correcting the errors of the previous ones. This sequential approach allows xgboost to create highly accurate models by iteratively refining predictions and addressing the weaknesses of earlier trees.\nImport the Machine Learning libraries we will use.\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n\nOur dataset has a label indicating which set (training or test), our data belong to. We wil use this to split it\n\n# recall groups\nclass_data_utm.groupby(['group']).size()\n\n\nclass_data_utm.crs\n\n\ndtrain = dsp.where(dsp['group']==1,drop=True)\ndtest = dsp.where(dsp['group']==2,drop=True)\n\n#create separte datasets for labels and features\ny_train = dtrain['class'].values.astype(int)\ny_test = dtest['class'].values.astype(int)\nX_train = dtrain['reflectance'].values\nX_test = dtest['reflectance'].values\n\n\nTrain ML model\nThe steps we will go through to train the model are:\nFirst, we define the hyperparameter grid. Initially, we set up a comprehensive grid (param_grid) with multiple values for several hyperparameters of the XGBoost model.\nNext, we create an XGBoost classifier object using the XGBClassifier class from the XGBoost library.\nWe then set up the GridSearchCV object using our defined XGBoost model and the hyperparameter grid. GridSearchCV allows us to perform an exhaustive search over the specified hyperparameter values to find the optimal combination that results in the best model performance. We choose a 5-fold cross-validation strategy (cv=5), meaning we split our training data into five subsets to validate the model’s performance across different data splits. We use accuracy as our scoring metric to evaluate the models.\nAfter setting up the grid search, we fit the GridSearchCV object to our training data (X_train and y_train). This process involves training multiple models with different hyperparameter combinations and evaluating their performance using cross-validation. Our goal is to identify the set of hyperparameters that yields the highest accuracy.\nOnce the grid search completes, we print out the best set of hyperparameters and the corresponding best score. The grid_search.best_params_ attribute provides the combination of hyperparameters that achieved the highest cross-validation accuracy, while the grid_search.best_score_ attribute shows the corresponding accuracy score. Finally, we extract the best model (best_model) from the grid search results. This model is trained with the optimal hyperparameters and is ready for making predictions or further analysis in our classification task.\nThis will take approx 30 seconds\n\n# Define the hyperparameter grid\nparam_grid = {\n    'max_depth': [5],\n    'learning_rate': [0.1],\n    'subsample': [0.75],\n    'n_estimators' : [50,100]\n}\n\n# Create the XGBoost model object\nxgb_model = xgb.XGBClassifier(tree_method='hist')\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# Print the best set of hyperparameters and the corresponding score\nprint(\"Best set of hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\nbest_model = grid_search.best_estimator_\n\n\n\n\nEvaluate model performance\nWe will use our best model to predict the classes of the test data Then, we calculate the F1 score using f1_score, which balances precision and recall, and print it to evaluate overall performance.\nNext, we assess how well the model performs for predicting Pine trees by calculating its precision and recall. Precision measures the accuracy of the positive predictions. It answers the question, “Of all the instances we labeled as Pines, how many were actually Pines?”. Recall measures the model’s ability to identify all actual positive instances. It answers the question, “Of all the actual Pines, how many did we correctly identify?”. You may also be familiar with the terms Users’ and Producers’ Accuracy. Precision = User’ Accuracy, and Recall = Producers’ Accuracy.\nFinally, we create and display a confusion matrix to visualize the model’s prediction accuracy across all classes\n\ny_pred = best_model.predict(X_test)\n\n# Step 2: Calculate acc and F1 score for the entire dataset\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {acc}\")\n\nf1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' accounts for class imbalance\nprint(f\"F1 Score (weighted): {f1}\")\n\n# Step 3: Calculate precision and recall for class 5 (Pine)\nprecision_class_5 = precision_score(y_test, y_pred, labels=[5], average='macro', zero_division=0)\nrecall_class_5 = recall_score(y_test, y_pred, labels=[5], average='macro', zero_division=0)\n\nprint(f\"Precision for Class 5: {precision_class_5}\")\nprint(f\"Recall for Class 5: {recall_class_5}\")\n\n# Step 4: Plot the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot()\nplt.show()\n\n\n\nSkipping Some steps in Glenn’s BioSCape Workshop Tutorial\n8.2.1.8. Interpret and understand ML model\nhttps://ornldaac.github.io/bioscape_workshop_sa/tutorials/Machine_Learning/Invasive_AVIRIS.html#interpret-and-understand-ml-model\n\n\nPredict over an example AVIRIS scene\nWe now have a trained model and are ready to deploy it to generate predictions across an entire AVIRIS scene and map the distribution of invasive plants. This involves handling a large volume of data, so we need to write the code to do this intelligently. We will accomplish this by applying the .predict() method of our trained model in parallel across the chunks of the AVIRIS xarray. The model will receive one chunk at a time so that the data is not too large, but it will be able to perform this operation in parallel across multiple chunks, and therefore will not take too long.\nThis model was only trained on data covering natural vegetaton in the Cape Peninsula, It is important that we only predict in the areas that match our training data. We will therefore filter to scenes that cover the Cape Peninsula and mask out non-protected areas\n\n#south africa protected areas\nSAPAD = (gpd.read_file('data/SAPAD_2024.gpkg')\n         .query(\"SITE_TYPE!='Marine Protected Area'\")\n        )\n#SAPAD.plot()\n#SAPAD.to_crs(\"EPSG:32734\")\nSAPAD.crs\n\n\n# Get the bounding box of the training data\nbbox = class_data_utm.total_bounds  # (minx, miny, maxx, maxy)\n#bbox\ngdf_bbox = gpd.GeoDataFrame({'geometry': [box(*bbox)]}, crs=class_data_utm.crs)  # Specify the CRS\ngdf_bbox['geometry'] = gdf_bbox.buffer(500)\ngdf_bbox.crs\n\n\n#south africa protected areas\nSAPAD = (gpd.read_file('data/SAPAD_2024.gpkg')\n         .query(\"SITE_TYPE!='Marine Protected Area'\")\n        )\nSAPAD = SAPAD.to_crs(\"EPSG:32734\")\n\n# Get the bounding box of the training data\nbbox = class_data_utm.total_bounds  # (minx, miny, maxx, maxy)\ngdf_bbox = gpd.GeoDataFrame({'geometry': [box(*bbox)]}, crs=class_data_utm.crs)  # Specify the CRS\ngdf_bbox['geometry'] = gdf_bbox.buffer(500)\n\n# protected areas that intersect with the training data\nSAPAD_CT = SAPAD.overlay(gdf_bbox,how='intersection')\n\n#keep only AVIRIS scenes that intersects with CT protected areas\nAVNG_sapad = AVNG_CP[AVNG_CP.intersects(SAPAD_CT.unary_union)]\n\n#a list of files to predict\nfiles_sapad = AVNG_sapad['RFL s3'].tolist()\n\n#how many files?\nlen(files_sapad)\n\n\nm = AVNG_sapad[['fid','geometry']].explore('fid')\nm\n\n\nSAPAD.keys()\n\n\n#map = AVNG_Coverage[['fid', 'geometry']].explore('fid')\nmap = SAPAD[['SITE_TYPE', 'geometry']].explore('SITE_TYPE')\nmap\n\nHere is the function that we will actually apply to each chunk. Simple really. The hard work is getting the data into and out of this functiON\n\ndef predict_on_chunk(chunk, model):\n    probabilities = model.predict_proba(chunk)\n    return probabilities\n\nNow we define the funciton that takes as input the path to the AVIRIS file and pass the data to the predict function. THhs is composed of 4 parts:\nPart 1: Opens the AVIRIS data file using xarray and sets a condition to identify valid data points where reflectance values are greater than zero.\nPart 2: Applies all the transformations that need to be done before the data goes to the model. It the spatial dimensions (x and y) into a single dimension, filters wavelengths, and normalizes the reflectance data.\nPart 3: Applies the machine learning model to the normalized data in parallel, predicting class probabilities for each data point using xarray’s apply_ufunc method. Most of the function invloves defining what to do with the dimensions of the old dataset and the new output\nPart 4: Unstacks the data to restore its original dimensions, sets spatial dimensions and coordinate reference system (CRS), clips the data, and transposes the data to match expected formats before returning the results.\n\ndef predict_xr(file,geometries):\n\n    #part 1 - opening file\n    #open the file\n    print(f'file: {file}')\n    ds = xr.open_datatree(rfl_netcdf_2i2c, engine='h5netcdf', decode_coords=\"all\",\n                         chunks='auto')\n\n    #get the geometries of the protected areas for masking\n    ds_crs = ds.transverse_mercator.crs_wkt\n    geometries = geometries.to_crs(ds_crs).geometry.apply(mapping)\n\n    #condition to use for masking no data later\n    condition = (ds['reflectance'] &gt; -1).any(dim='wavelength')\n\n    #stack the data into a single dimension. This will be important for applying the model later\n    ds = ds.reflectance.to_dataset().stack(sample=('easting','northing'))\n    \n    #part 2 - pre-processing\n    #remove bad wavelenghts\n    wavelengths_to_drop = ds.wavelength.where(\n        (ds.wavelength &lt; 450) |\n        (ds.wavelength &gt;= 1340) & (ds.wavelength &lt;= 1480) |\n        (ds.wavelength &gt;= 1800) & (ds.wavelength &lt;= 1980) |\n        (ds.wavelength &gt; 2400), drop=True\n    )\n    # Use drop_sel() to remove those specific wavelength ranges\n    ds = ds.drop_sel(wavelength=wavelengths_to_drop)\n    \n    #normalise the data\n    l2_norm = np.sqrt((ds['reflectance'] ** 2).sum(dim='wavelength'))\n    ds['reflectance'] = ds['reflectance'] / l2_norm\n\n     \n    #part 3 - apply the model over chunks\n    result = xr.apply_ufunc(\n        predict_on_chunk,\n        ds['reflectance'].chunk(dict(wavelength=-1)),\n        input_core_dims=[['wavelength']],#input dim with features\n        output_core_dims=[['class']],  # name for the new output dim\n        exclude_dims=set(('wavelength',)),  #dims to drop in result\n        output_sizes={'class': 9}, #length of the new dimension\n        output_dtypes=[np.float32],\n        dask=\"parallelized\",\n        kwargs={'model': best_model}\n    )\n\n    #part 4 - post-processing\n    result = result.where((result &gt;= 0) & (result &lt;= 1), np.nan) #valid values\n    result = result.unstack('sample') #remove the stack\n    result = result.rio.set_spatial_dims(x_dim='easting',y_dim='northing') #set the spatial dims\n    result = result.rio.write_crs(ds_crs) #set the CRS\n    result = result.rio.clip(geometries) #clip to the protected areas and no data\n    result = result.transpose('class', 'northing', 'easting') #transpose the data rio expects it this way\n    return result   \n\nLet’s test that it works on a single file before we run it through 100s of GB of data.\n\n#files_sapad[25]\n\n\ntest  = predict_xr(rfl_netcdf_2i2c,SAPAD_CT)\ntest\n\n\nlabel_df\n\n\ntest = test.rio.reproject(\"EPSG:4326\",nodata=np.nan)\nh = test.isel({'class':5}).hvplot(tiles=hv.element.tiles.EsriImagery(), \n                              project=True,rasterize=True,clim=(0,1),\n                              cmap='magma',frame_width=400,data_aspect=1,alpha=0.5)\nh\n\nML models typically provide a single prediction of the most likely outcomes. You can also get probability-like scores (values from 0 to 1) from these models, but they are not true probabilities. If the model gives you a score of 0.6, that means it is more likely than a prediction of 0.5, and less likely than 0.7. However, it does not mean that in a large sample your prediction would be right 60 times out of 100. To get calibrated probabilities from our models, we have to apply additional steps. We can also get a set of predictions from models rather than a single prediction, which reflects the model’s true uncertainty using a technique called conformal predictions. Read more about conformal prediction for geospatial machine learning in this amazing paper:\nSingh, G., Moncrieff, G., Venter, Z., Cawse-Nicholson, K., Slingsby, J., & Robinson, T. B. (2024). Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction. Scientific Reports, 14(1), 16166.\n\n\nFinal steps of the full ML classification are time intensive and are not described in this workshop.\nSteps in Glenn Moncrieff’s BioSCape Workshop Tutorial\n8.2.1.10. Merge and mosaic results - https://ornldaac.github.io/bioscape_workshop_sa/tutorials/Machine_Learning/Invasive_AVIRIS.html#merge-and-mosaic-results\n\n\nCREDITS:\nFind all of the October 2025 BioSCape Data Workshop Materials/Notebooks\n\nhttps://ornldaac.github.io/bioscape_workshop_sa/intro.html\n\nThis Notebook is an adaption of Glenn Moncrieff’s BioSCape Data Workshop Notebook: Mapping invasive species using supervised machine learning and AVIRIS-NG - This Notebook accesses and uses an updated version of AVIRIS-NG data with improved corrections and that are in netCDF file formats\nGlenn’s lesson borrowed from:\n\nLand cover mapping example on Microsoft Planetary Computer",
    "crumbs": [
      "Mapping invasive species using AVIRIS-NG",
      "Machine learning with AVIRIS-NG"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioSCape Workshop at BioSpace25",
    "section": "",
    "text": "Workshop Description\nHarnessing analysis tools for biodiversity applications using field, airborne, and orbital remote sensing data from NASA’s BioSCAPE campaign\nMichele Thornton1, Rupesh Shrestha1, Erin Hestir2, Adam Wilson3, Jasper Slingsby4, Anabelle Cardoso3,4\n1Oak Ridge National Laboratory Distributed Active Archive Center; 2University of California, Merced; 3University at Buffalo; 4University of Cape Town\nIn October/November of 2023, the US National Aeronautics and Space Administration (NASA) conducted its first Biodiversity field and airborne campaign across terrestrial and aquatic environments in the South African Greater Cape Floristic Region (GCFR). From 4 airborne instruments (Airborne Visible-Infrared Imaging Spectrometer - Next Generation (AVIRIS-NG), Portable Remote Imaging SpectroMeter (PRISM), Hyperspectral Thermal Emission Spectrometer (HyTES), and Land, Vegetation, and Ice Sensor (LVIS)) the BioSCape Campaign’s remote sensing data products provides an unprecedented level of image spectroscopy from VSWIR to TIR wavelengths as well as full-waveform laser altimeter measurements. Airborne data are supplemented with a rich combination of contemporaneous biodiversity-relevant field observations toward an approach to measure and understand functional, phylogenetic, and taxonomic biological diversity as components of ecosystem function.\nA majority of the BioSCape Campaign data will be archived through the NASA-funded Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC). The discipline-specific Center provides dataset content to NASA’s Earthdata Cloud which includes a standardized metadata called Common Metadata Repository (CMR), data discovery, and open access.\nThis hands-on demonstration will leverage a managed cloud environment to show programmatic discovery, access, and analysis of NASA BioSCape data/resources and concurrent orbital data. Included will be content and tutorials demonstrating derivation of estimates of biodiversity variables including:\n\nAn overview of the BioSCape Campaign data acquisition\nNASA Earthdata Cloud: Search, Access, and Analysis Basics\nExplore BioSCape vegetation plot and image spectroscopy data\nInvasive species analysis from AVIRIS-NG and Vegetation Plot Data\nVegetation Structural Diversity derived from LVIS and GEDI full waveform lidar data.\nAquatic biodiversity estimates from PRISM, PACE, and EMIT\n\nGET STARTED:\n\nDeploy Jupyter Lab instance in 2i2c",
    "crumbs": [
      "Welcome",
      "BioSCape Workshop at BioSpace25"
    ]
  },
  {
    "objectID": "tutorials/prerequisites.html",
    "href": "tutorials/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2 at https://workshop.openscapes.2i2c.cloud/hub/. During the workshop, log in using your email address as a username and the password we will share during the workshop.",
    "crumbs": [
      "Welcome",
      "Prerequisites"
    ]
  },
  {
    "objectID": "tutorials/prerequisites.html#prerequisites",
    "href": "tutorials/prerequisites.html#prerequisites",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2 at https://workshop.openscapes.2i2c.cloud/hub/. During the workshop, log in using your email address as a username and the password we will share during the workshop.",
    "crumbs": [
      "Welcome",
      "Prerequisites"
    ]
  },
  {
    "objectID": "tutorials/avirisng/GettingStarted.html",
    "href": "tutorials/avirisng/GettingStarted.html",
    "title": "Welcome to the BIOSPACE25 BioSCape Workshop",
    "section": "",
    "text": "Michele Thornton, Rupesh Shrestha, Erin Hestir, Adam Wilson, Jasper Slingsby, Anabelle Cardoso\nDate: February 12, 2025, Frascati (Rome), Italy\n\n\n\nBIOSPACE25"
  },
  {
    "objectID": "tutorials/avirisng/GettingStarted.html#harnessing-analysis-tools-for-biodiversity-applications-using-field-airborne-and-orbital-remote-sensing-data-from-nasas-bioscape-campaign",
    "href": "tutorials/avirisng/GettingStarted.html#harnessing-analysis-tools-for-biodiversity-applications-using-field-airborne-and-orbital-remote-sensing-data-from-nasas-bioscape-campaign",
    "title": "Welcome to the BIOSPACE25 BioSCape Workshop",
    "section": "",
    "text": "Michele Thornton, Rupesh Shrestha, Erin Hestir, Adam Wilson, Jasper Slingsby, Anabelle Cardoso\nDate: February 12, 2025, Frascati (Rome), Italy\n\n\n\nBIOSPACE25"
  },
  {
    "objectID": "tutorials/avirisng/GettingStarted.html#getting-started",
    "href": "tutorials/avirisng/GettingStarted.html#getting-started",
    "title": "Welcome to the BIOSPACE25 BioSCape Workshop",
    "section": "Getting Started",
    "text": "Getting Started\n\n1. If you do not already have and know your NASA Earthdata Login Account, please get one here:\n\nhttps://urs.earthdata.nasa.gov/\n\n\n\n2. Download an AVIRIS-NG Reflectance file from JPL.\n\nrun the wget code block below\n\n\n!wget https://popo.jpl.nasa.gov/pub/bioscape_netCDF/rfl/ang20231109t133124_005_L2A_OE_0b4f48b4_RFL_ORT.nc -P /home/jovyan/2025-biospace/tutorials/avirisng/data/ang"
  }
]